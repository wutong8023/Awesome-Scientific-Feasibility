% Example: modify keywords by the way you like.
% Note: the keywords should be predefined in scripts/bibtext_to_md.py # fined_taxonomy
% ---------------- Preprint ----------------
@inproceedings{PradeepMNL21,
  author       = {Ronak Pradeep and
                  Xueguang Ma and
                  Rodrigo Frassetto Nogueira and
                  Jimmy Lin},
  title        = {Scientific Claim Verification with VerT5erini},
  booktitle    = {Proceedings of LOUHI@EACL},
  pages        = {94--103},
  year         = {2021},
  url          = {https://www.aclweb.org/anthology/2021.louhi-1.11/},
  keywords = {
        Method, Resource,
        Biomedical,
        Others RQs,
        T5,
        Other Data Format,
    },
}
@String(PradeepMNL21="We propose a system called VerT5erini that exploits T5 for abstract retrieval, sentence selection, and label prediction, which are three critical sub-tasks of claim verification. We evaluate our pipeline on SciFACT, a newly curated dataset that requires models to not just predict the veracity of claims but also provide relevant sentences from a corpus of scientific literature that support the prediction. ")


@inproceedings{WaddenLLWZCH20,
  author       = {David Wadden and
                  Shanchuan Lin and
                  Kyle Lo and
                  Lucy Lu Wang and
                  Madeleine van Zuylen and
                  Arman Cohan and
                  Hannaneh Hajishirzi},
  title        = {Fact or Fiction: Verifying Scientific Claims},
  booktitle    = {Proceedings of EMNLP},
  pages        = {7534--7550},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.609},
  keywords = {
        Method,
        Other Area,
        Others RQs,
        Other Model,
        Other Data Format
    },
}
@String(WaddenLLWZCH20="We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. ")

@inproceedings{WaddenLWCBH22,
  author       = {David Wadden and
                  Kyle Lo and
                  Lucy Lu Wang and
                  Arman Cohan and
                  Iz Beltagy and
                  Hannaneh Hajishirzi},
  title        = {MultiVerS: Improving scientific claim verification with weak supervision
                  and full-document context},
  booktitle    = {Findings of NAACL},
  pages        = {61--76},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.findings-naacl.6},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(WaddenLWCBH22="We introduce MultiVerS, a weakly supervised model for scientific claim verification that leverages full-document context. We show that MultiVerS outperforms strong supervised baselines on the SciFact dataset, achieving state-of-the-art performance on this task. ")

@article{abs-2107-08188,
  author       = {David Wadden and
                  Kyle Lo},
  title        = {Overview and Insights from the SciVer Shared Task on Scientific Claim
                  Verification},
  journal      = {CoRR},
  volume       = {abs/2107.08188},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.08188},
    keywords = {
            Survey,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(abs-2107-08188="We present an overview of the SciVer shared task on scientific claim verification, which was held as part of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). The shared task aimed to promote research on scientific claim verification, a new task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. ")

@inproceedings{VladikaM23,
  author       = {Juraj Vladika and
                  Florian Matthes},
  title        = {Scientific Fact-Checking: {A} Survey of Resources and Approaches},
  booktitle    = {Findings of ACL},
  pages        = {6215--6230},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.findings-acl.387},
    keywords = {
            Survey,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(VladikaM23="We present a survey of resources and approaches for scientific fact-checking, a task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. ")

@inproceedings{ZhangLFY21,
  author       = {Zhiwei Zhang and
                  Jiyi Li and
                  Fumiyo Fukumoto and
                  Yanming Ye},
  title        = {Abstract, Rationale, Stance: {A} Joint Model for Scientific Claim
                  Verification},
  booktitle    = {Proceedings of EMNLP},
  pages        = {3580--3586},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.emnlp-main.290},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(ZhangLFY21="We propose a joint model for scientific claim verification that predicts the stance of a given scientific claim by jointly learning to predict the abstract and rationale that support the stance. ")

@inproceedings{WaddenLKCBWH22,
  author       = {David Wadden and
                  Kyle Lo and
                  Bailey Kuehl and
                  Arman Cohan and
                  Iz Beltagy and
                  Lucy Lu Wang and
                  Hannaneh Hajishirzi},
  title        = {SciFact-Open: Towards open-domain scientific claim verification},
  booktitle    = {Findings of EMNLP},
  pages        = {4719--4734},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.findings-emnlp.347},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(WaddenLKCBWH22="We introduce SciFact-Open, a new dataset for open-domain scientific claim verification. SciFact-Open contains 1,000 scientific claims and 10,000 abstracts from the research literature, and requires models to select abstracts containing evidence that SUPPORTS or REFUTES a given scientific claim. ")

@article{abs-2202-02646,
  author       = {Ashish Rana and
                  Deepanshu Khanna and
                  Muskaan Singh and
                  Tirthankar Ghosal and
                  Harpreet Singh and
                  Prashant Singh Rana},
  title        = {RerrFact: Reduced Evidence Retrieval Representations for Scientific
                  Claim Verification},
  journal      = {CoRR},
  volume       = {abs/2202.02646},
  year         = {2022},
  url          = {https://arxiv.org/abs/2202.02646},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(abs-2202-02646="We propose a reduced evidence retrieval model for scientific claim verification that uses a small number of retrieved abstracts to make predictions. ")

@article{abs-2104-11572,
  author       = {Xia Zeng and
                  Arkaitz Zubiaga},
  title        = {{QMUL-SDS} at {SCIVER:} Step-by-Step Binary Classification for Scientific
                  Claim Verification},
  journal      = {CoRR},
  volume       = {abs/2104.11572},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.11572},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(abs-2104-11572="We present a step-by-step binary classification model for scientific claim verification that predicts the stance of a given scientific claim by learning to predict the abstract and rationale that support the stance. ")


@inproceedings{abs-2402-02844,
  author       = {Juraj Vladika and
                  Florian Matthes},
  title        = {Comparing Knowledge Sources for Open-Domain Scientific Claim Verification},
  booktitle    = {Proceedings of EACL},
  pages        = {4719--4734},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.02844},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(abs-2402-02844="We compare knowledge sources for open-domain scientific claim verification, a task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. ")

@inproceedings{0001WLKCAW22,
  author       = {Dustin Wright and
                  David Wadden and
                  Kyle Lo and
                  Bailey Kuehl and
                  Arman Cohan and
                  Isabelle Augenstein and
                  Lucy Lu Wang},
  title        = {Generating Scientific Claims for Zero-Shot Scientific Fact Checking},
  booktitle    = {Proceedings of ACL},
  pages        = {2448--2460},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.acl-long.175},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}

@inproceedings{WangS23a,
  author       = {Haoran Wang and
                  Kai Shu},
  title        = {Explainable Claim Verification via Knowledge-Grounded Reasoning with
                  Large Language Models},
  booktitle    = {Findings of EMNLP},
  pages        = {6288--6304},
  year         = {2023},
  url          = {https://aclanthology.org/2023.findings-emnlp.416},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(WangS23a="We propose a knowledge-grounded reasoning model for scientific claim verification that uses large language models to generate explanations for the predictions. ")

@article{abs-2310-09754,
  author       = {Huanhuan Ma and
                  Weizhi Xu and
                  Yifan Wei and
                  Liuji Chen and
                  Liang Wang and
                  Qiang Liu and
                  Shu Wu and
                  Liang Wang},
  title        = {{EX-FEVER:} {A} Dataset for Multi-hop Explainable Fact Verification},
  journal      = {CoRR},
  volume       = {abs/2310.09754},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.09754},
    keywords = {
            Resource,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(abs-2310-09754="We present EX-FEVER, a new dataset for multi-hop explainable fact verification. EX-FEVER contains 10,000 claims and 100,000 abstracts from the research literature, and requires models to select abstracts containing evidence that SUPPORTS or REFUTES a given scientific claim. ")

@inproceedings{LuPLNK23,
  author       = {Xinyuan Lu and
                  Liangming Pan and
                  Qian Liu and
                  Preslav Nakov and
                  Min{-}Yen Kan},
  title        = {{SCITAB:} {A} Challenging Benchmark for Compositional Reasoning and
                  Claim Verification on Scientific Tables},
  booktitle    = {Proceedings of EMNLP},
  pages        = {7787--7813},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.483},
    keywords = {
            Resource,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(LuPLNK23="We present SCITAB, a new benchmark for compositional reasoning and claim verification on scientific tables. SCITAB contains 10,000 claims and 100,000 tables from the research literature, and requires models to select tables containing evidence that SUPPORTS or REFUTES a given scientific claim. ")

@article{abs-2305-11859,
  author       = {Jifan Chen and
                  Grace Kim and
                  Aniruddh Sriram and
                  Greg Durrett and
                  Eunsol Choi},
  title        = {Complex Claim Verification with Evidence Retrieved in the Wild},
  journal      = {CoRR},
  volume       = {abs/2305.11859},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.11859},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}

@inproceedings{HuHGWY23,
  author       = {Xuming Hu and
                  Zhaochen Hong and
                  Zhijiang Guo and
                  Lijie Wen and
                  Philip S. Yu},
  title        = {Read it Twice: Towards Faithfully Interpretable Fact Verification
                  by Revisiting Evidence},
  booktitle    = {Proceedings of SIGIR},
  pages        = {2319--2323},
  year         = {2023},
  url          = {https://doi.org/10.1145/3539618.3592049},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}


@article{abs-2304-02769,
  author       = {Viswanath Chadalapaka and
                  Derek Nguyen and
                  JoonWon Choi and
                  Shaunak Joshi and
                  Mohammad Rostami},
  title        = {Low-Shot Learning for Fictional Claim Verification},
  journal      = {CoRR},
  volume       = {abs/2304.02769},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.02769},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(abs-2304-02769="We propose a low-shot learning model for fictional claim verification that uses a small number of examples to make predictions. ")

@inproceedings{MaGJW19,
  author       = {Jing Ma and
                  Wei Gao and
                  Shafiq R. Joty and
                  Kam{-}Fai Wong},
  title        = {Sentence-Level Evidence Embedding for Claim Verification with Hierarchical
                  Attention Networks},
  booktitle    = {Proceedings of ACL},
  pages        = {2561--2571},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1244},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(MaGJW19="We propose a hierarchical attention network for scientific claim verification that uses sentence-level evidence embeddings to make predictions. ")

@article{abs-2301-08914,
  author       = {Sai Gurrapu and
                  Lifu Huang and
                  Feras A. Batarseh},
  title        = {ExClaim: Explainable Neural Claim Verification Using Rationalization},
  journal      = {CoRR},
  volume       = {abs/2301.08914},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2301.08914},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(abs-2301-08914="We propose an explainable neural claim verification model that uses rationalization to generate explanations for the predictions. ")

@inproceedings{YaoS0CH23,
  author       = {Barry Menglong Yao and
                  Aditya Shah and
                  Lichao Sun and
                  Jin{-}Hee Cho and
                  Lifu Huang},
  title        = {End-to-End Multimodal Fact-Checking and Explanation Generation: {A}
                  Challenging Dataset and Models},
  booktitle    = {Proceedings of SIGIR},
  pages        = {2733--2743},
  year         = {2023},
  url          = {https://doi.org/10.1145/3539618.3591879},
    keywords = {
            Method,
            Other Area,
            Others RQs,
            Other Model,
            Other Data Format
        },
}
@String(YaoS0CH23="We present a challenging dataset and models for end-to-end multimodal fact-checking and explanation generation. ")
















