# Scientific Feasibility (SciFy) Literature 
This repository is maintained by [Tongtong Wu](https://wutong8023.site). Please don't hesitate to send me an email to collaborate or fix some entries (wutong8023 AT gmail.com). 
The automation script of this repo is powered by [Auto-Bibfile](https://github.com/wutong8023/Auto-Bibfile.git).

You can directly use our bibtex.bib in overleaf with this [link](https://www.overleaf.com/read/rgscdxhxbwhp).

This page categorizes the literature by the **Top Author**.

## Outline 
- [![](https://img.shields.io/badge/Hyperlink-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#hyperlink)
- [![](https://img.shields.io/badge/Kyle_Lo-5-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#kyle-lo)
- [![](https://img.shields.io/badge/David_Wadden-5-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#david-wadden)
- [![](https://img.shields.io/badge/Arman_Cohan-4-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#arman-cohan)
- [![](https://img.shields.io/badge/Lucy_Lu_Wang-4-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#lucy-lu-wang)
- [![](https://img.shields.io/badge/Charles_Kemp-3-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#charles-kemp)
- [![](https://img.shields.io/badge/Hannaneh_Hajishirzi-3-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#hannaneh-hajishirzi)
- [![](https://img.shields.io/badge/Lifu_Huang-2-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#lifu-huang)
- [![](https://img.shields.io/badge/Liang_Wang-1-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#liang-wang)
- [![](https://img.shields.io/badge/Bailey_Kuehl-2-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#bailey-kuehl)
- [![](https://img.shields.io/badge/Florian_Matthes-2-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#florian-matthes)
- [![](https://img.shields.io/badge/Juraj_Vladika-2-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#juraj-vladika)
- [![](https://img.shields.io/badge/Iz_Beltagy-2-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author/README.md#iz-beltagy)
## Hyperlink 
 - [Overview](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/./)
 - [Application Area](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/application)
 - [Top Author](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author)
 - [Contribution](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/contribution)
 - [Dataset Format](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/dataset)
 - [Foundation Model](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/foundation_model)
 - [Research Question](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/research_question)
 - [Published Time](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/time)
 - [Published Venue](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue)

## Kyle Lo

- [![](https://img.shields.io/badge/Findings_of_NAACL-2022-blue)](https://doi.org/10.18653/v1/2022.findings-naacl.6) [**MultiVerS: Improving scientific claim verification with weak supervision
and full-document context**](https://doi.org/10.18653/v1/2022.findings-naacl.6) , <br> by *David Wadden and
Kyle Lo and
Lucy Lu Wang and
Arman Cohan and
Iz Beltagy and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L48-L68)<br> ```We introduce MultiVerS, a weakly supervised model for scientific claim verification that leverages full-document context. We show that MultiVerS outperforms strong supervised baselines on the SciFact dataset, achieving state-of-the-art performance on this task. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLWCBH22```<br>

- [![](https://img.shields.io/badge/Findings_of_EMNLP-2022-blue)](https://doi.org/10.18653/v1/2022.findings-emnlp.347) [**SciFact-Open: Towards open-domain scientific claim verification**](https://doi.org/10.18653/v1/2022.findings-emnlp.347) , <br> by *David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Iz Beltagy and
Lucy Lu Wang and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L129-L149)<br> ```Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLKCBWH22```<br>

- [![](https://img.shields.io/badge/ACL-2022-blue)](https://doi.org/10.18653/v1/2022.acl-long.175) [**Generating Scientific Claims for Zero-Shot Scientific Fact Checking**](https://doi.org/10.18653/v1/2022.acl-long.175) , <br> by *Dustin Wright and
David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Isabelle Augenstein and
Lucy Lu Wang* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L213-L233)<br> </details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```0001WLKCAW22```<br>

- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2107.08188) [**Overview and Insights from the SciVer Shared Task on Scientific Claim
Verification**](https://arxiv.org/abs/2107.08188) , <br> by *David Wadden and
Kyle Lo* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L71-L87)<br> ```We present an overview of the SciVer shared task on scientific claim verification, which was held as part of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). The shared task aimed to promote research on scientific claim verification, a new task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2107-08188```<br>

- [![](https://img.shields.io/badge/EMNLP-2020-blue)](https://doi.org/10.18653/v1/2020.emnlp-main.609) [**Fact or Fiction: Verifying Scientific Claims**](https://doi.org/10.18653/v1/2020.emnlp-main.609) , <br> by *David Wadden and
Shanchuan Lin and
Kyle Lo and
Lucy Lu Wang and
Madeleine van Zuylen and
Arman Cohan and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L25-L45)<br> ```We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLLWZCH20```<br>

## David Wadden

- [![](https://img.shields.io/badge/Findings_of_NAACL-2022-blue)](https://doi.org/10.18653/v1/2022.findings-naacl.6) [**MultiVerS: Improving scientific claim verification with weak supervision
and full-document context**](https://doi.org/10.18653/v1/2022.findings-naacl.6) , <br> by *David Wadden and
Kyle Lo and
Lucy Lu Wang and
Arman Cohan and
Iz Beltagy and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L48-L68)<br> ```We introduce MultiVerS, a weakly supervised model for scientific claim verification that leverages full-document context. We show that MultiVerS outperforms strong supervised baselines on the SciFact dataset, achieving state-of-the-art performance on this task. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLWCBH22```<br>

- [![](https://img.shields.io/badge/Findings_of_EMNLP-2022-blue)](https://doi.org/10.18653/v1/2022.findings-emnlp.347) [**SciFact-Open: Towards open-domain scientific claim verification**](https://doi.org/10.18653/v1/2022.findings-emnlp.347) , <br> by *David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Iz Beltagy and
Lucy Lu Wang and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L129-L149)<br> ```Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLKCBWH22```<br>

- [![](https://img.shields.io/badge/ACL-2022-blue)](https://doi.org/10.18653/v1/2022.acl-long.175) [**Generating Scientific Claims for Zero-Shot Scientific Fact Checking**](https://doi.org/10.18653/v1/2022.acl-long.175) , <br> by *Dustin Wright and
David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Isabelle Augenstein and
Lucy Lu Wang* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L213-L233)<br> </details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```0001WLKCAW22```<br>

- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2107.08188) [**Overview and Insights from the SciVer Shared Task on Scientific Claim
Verification**](https://arxiv.org/abs/2107.08188) , <br> by *David Wadden and
Kyle Lo* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L71-L87)<br> ```We present an overview of the SciVer shared task on scientific claim verification, which was held as part of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). The shared task aimed to promote research on scientific claim verification, a new task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2107-08188```<br>

- [![](https://img.shields.io/badge/EMNLP-2020-blue)](https://doi.org/10.18653/v1/2020.emnlp-main.609) [**Fact or Fiction: Verifying Scientific Claims**](https://doi.org/10.18653/v1/2020.emnlp-main.609) , <br> by *David Wadden and
Shanchuan Lin and
Kyle Lo and
Lucy Lu Wang and
Madeleine van Zuylen and
Arman Cohan and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L25-L45)<br> ```We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLLWZCH20```<br>

## Arman Cohan

- [![](https://img.shields.io/badge/Findings_of_NAACL-2022-blue)](https://doi.org/10.18653/v1/2022.findings-naacl.6) [**MultiVerS: Improving scientific claim verification with weak supervision
and full-document context**](https://doi.org/10.18653/v1/2022.findings-naacl.6) , <br> by *David Wadden and
Kyle Lo and
Lucy Lu Wang and
Arman Cohan and
Iz Beltagy and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L48-L68)<br> ```We introduce MultiVerS, a weakly supervised model for scientific claim verification that leverages full-document context. We show that MultiVerS outperforms strong supervised baselines on the SciFact dataset, achieving state-of-the-art performance on this task. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLWCBH22```<br>

- [![](https://img.shields.io/badge/Findings_of_EMNLP-2022-blue)](https://doi.org/10.18653/v1/2022.findings-emnlp.347) [**SciFact-Open: Towards open-domain scientific claim verification**](https://doi.org/10.18653/v1/2022.findings-emnlp.347) , <br> by *David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Iz Beltagy and
Lucy Lu Wang and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L129-L149)<br> ```Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLKCBWH22```<br>

- [![](https://img.shields.io/badge/ACL-2022-blue)](https://doi.org/10.18653/v1/2022.acl-long.175) [**Generating Scientific Claims for Zero-Shot Scientific Fact Checking**](https://doi.org/10.18653/v1/2022.acl-long.175) , <br> by *Dustin Wright and
David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Isabelle Augenstein and
Lucy Lu Wang* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L213-L233)<br> </details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```0001WLKCAW22```<br>

- [![](https://img.shields.io/badge/EMNLP-2020-blue)](https://doi.org/10.18653/v1/2020.emnlp-main.609) [**Fact or Fiction: Verifying Scientific Claims**](https://doi.org/10.18653/v1/2020.emnlp-main.609) , <br> by *David Wadden and
Shanchuan Lin and
Kyle Lo and
Lucy Lu Wang and
Madeleine van Zuylen and
Arman Cohan and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L25-L45)<br> ```We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLLWZCH20```<br>

## Lucy Lu Wang

- [![](https://img.shields.io/badge/Findings_of_NAACL-2022-blue)](https://doi.org/10.18653/v1/2022.findings-naacl.6) [**MultiVerS: Improving scientific claim verification with weak supervision
and full-document context**](https://doi.org/10.18653/v1/2022.findings-naacl.6) , <br> by *David Wadden and
Kyle Lo and
Lucy Lu Wang and
Arman Cohan and
Iz Beltagy and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L48-L68)<br> ```We introduce MultiVerS, a weakly supervised model for scientific claim verification that leverages full-document context. We show that MultiVerS outperforms strong supervised baselines on the SciFact dataset, achieving state-of-the-art performance on this task. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLWCBH22```<br>

- [![](https://img.shields.io/badge/Findings_of_EMNLP-2022-blue)](https://doi.org/10.18653/v1/2022.findings-emnlp.347) [**SciFact-Open: Towards open-domain scientific claim verification**](https://doi.org/10.18653/v1/2022.findings-emnlp.347) , <br> by *David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Iz Beltagy and
Lucy Lu Wang and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L129-L149)<br> ```Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLKCBWH22```<br>

- [![](https://img.shields.io/badge/ACL-2022-blue)](https://doi.org/10.18653/v1/2022.acl-long.175) [**Generating Scientific Claims for Zero-Shot Scientific Fact Checking**](https://doi.org/10.18653/v1/2022.acl-long.175) , <br> by *Dustin Wright and
David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Isabelle Augenstein and
Lucy Lu Wang* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L213-L233)<br> </details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```0001WLKCAW22```<br>

- [![](https://img.shields.io/badge/EMNLP-2020-blue)](https://doi.org/10.18653/v1/2020.emnlp-main.609) [**Fact or Fiction: Verifying Scientific Claims**](https://doi.org/10.18653/v1/2020.emnlp-main.609) , <br> by *David Wadden and
Shanchuan Lin and
Kyle Lo and
Lucy Lu Wang and
Madeleine van Zuylen and
Arman Cohan and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L25-L45)<br> ```We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLLWZCH20```<br>

## Charles Kemp

- [![](https://img.shields.io/badge/Cogn._Syst._Res.-2024-blue)](https://doi.org/10.1016/j.cogsys.2023.101155) [**Inductive reasoning in humans and large language models**](https://doi.org/10.1016/j.cogsys.2023.101155) , <br> by *Simon Jerome Han and
Keith J. Ransom and
Andrew Perfors and
Charles Kemp* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L454-L472)<br> ```Apply GPT-3.5 and GPT-4 to a classic inductive reasoning task known as property induction. GPT-3.5 struggles to capture many aspects of human behavior. GPT-4’s performance qualitatively matches that of humans, aside from the phenomenon of premise non-monotonicity. Provide two large property induction datasets that can serve as future benchmarks.
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```HanRPK24```<br>

- [![](https://img.shields.io/badge/Psychological_review-2015-blue)](https://psycnet.apa.org/record/2015-45017-003) [**An improved probabilistic account of counterfactual reasoning.**](https://psycnet.apa.org/record/2015-45017-003) , <br> by *Lucas, Christopher G and Kemp, Charles* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L475-L491)<br> ```When people want to identify the causes of an event, assign credit or blame, or learn from their mistakes, they often reflect on how things could have gone differently. In this kind of reasoning, one considers a counterfactual world in which some events are different from their real-world counterparts and considers what else would have changed. Researchers have recently proposed several probabilistic models that aim to capture how people do (or should) reason about counterfactuals. We present a new model and show that it accounts better for human inferences than several alternative models. Our model builds on the work of Pearl (2000), and extends his approach in a way that accommodates backtracking inferences and that acknowledges the difference between counterfactual interventions and counterfactual observations. We present 6 new experiments and analyze data from 4 experiments carried out by Rips (2010), and the results suggest that the new model provides an accurate account of both mean human judgments and the judgments of individuals.(PsycINFO Database Record (c) 2016 APA, all rights reserved)
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```lucas2015improved```<br>

- [![](https://img.shields.io/badge/Psychonomic_bulletin_and_review-2014-blue)](https://www.charleskemp.com/papers/kempj_ataxonomyofinductiveproblems.pdf) [**A taxonomy of inductive problems**](https://www.charleskemp.com/papers/kempj_ataxonomyofinductiveproblems.pdf) , <br> by *Kemp, Charles and Jern, Alan* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L436-L451)<br> ```Inductive inferences about objects, features, categories, and relations have been studied for many years, but thereare few attempts to chart the range of inductive problems that humans are able to solve. We present a taxonomy of inductive problems that helps to clarify the relationships between familiar inductive problems such as generalization, categorization, and identification, and that introduces new inductive problems for psychological investigation. Our taxonomy is founded on the idea that semantic knowledge is organized into systems of objects, features, categories, and relations, and we attempt to characterize all of the inductive problems that can arise when these systems are partially observed. Recent studies have begun to address some of the new problems in our taxonomy, and future work should aim to develop unified theories of inductive reasoning that explain how people solve all of the problems in the taxonomy
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```kemp2014taxonomy```<br>

## Hannaneh Hajishirzi

- [![](https://img.shields.io/badge/Findings_of_NAACL-2022-blue)](https://doi.org/10.18653/v1/2022.findings-naacl.6) [**MultiVerS: Improving scientific claim verification with weak supervision
and full-document context**](https://doi.org/10.18653/v1/2022.findings-naacl.6) , <br> by *David Wadden and
Kyle Lo and
Lucy Lu Wang and
Arman Cohan and
Iz Beltagy and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L48-L68)<br> ```We introduce MultiVerS, a weakly supervised model for scientific claim verification that leverages full-document context. We show that MultiVerS outperforms strong supervised baselines on the SciFact dataset, achieving state-of-the-art performance on this task. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLWCBH22```<br>

- [![](https://img.shields.io/badge/Findings_of_EMNLP-2022-blue)](https://doi.org/10.18653/v1/2022.findings-emnlp.347) [**SciFact-Open: Towards open-domain scientific claim verification**](https://doi.org/10.18653/v1/2022.findings-emnlp.347) , <br> by *David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Iz Beltagy and
Lucy Lu Wang and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L129-L149)<br> ```Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLKCBWH22```<br>

- [![](https://img.shields.io/badge/EMNLP-2020-blue)](https://doi.org/10.18653/v1/2020.emnlp-main.609) [**Fact or Fiction: Verifying Scientific Claims**](https://doi.org/10.18653/v1/2020.emnlp-main.609) , <br> by *David Wadden and
Shanchuan Lin and
Kyle Lo and
Lucy Lu Wang and
Madeleine van Zuylen and
Arman Cohan and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L25-L45)<br> ```We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLLWZCH20```<br>

## Lifu Huang

- [![](https://img.shields.io/badge/CoRR-2023-blue)](https://doi.org/10.48550/arXiv.2301.08914) [**ExClaim: Explainable Neural Claim Verification Using Rationalization**](https://doi.org/10.48550/arXiv.2301.08914) , <br> by *Sai Gurrapu and
Lifu Huang and
Feras A. Batarseh* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L386-L402)<br> ```We propose an explainable neural claim verification model that uses rationalization to generate explanations for the predictions. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2301-08914```<br>

- [![](https://img.shields.io/badge/SIGIR-2023-blue)](https://doi.org/10.1145/3539618.3591879) [**End-to-End Multimodal Fact-Checking and Explanation Generation: A
Challenging Dataset and Models**](https://doi.org/10.1145/3539618.3591879) , <br> by *Barry Menglong Yao and
Aditya Shah and
Lichao Sun and
Jin{-}Hee Cho and
Lifu Huang* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L405-L424)<br> ```We present a challenging dataset and models for end-to-end multimodal fact-checking and explanation generation. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```YaoS0CH23```<br>

## Liang Wang

- [![](https://img.shields.io/badge/CoRR-2023-blue)](https://doi.org/10.48550/arXiv.2310.09754) [**EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification**](https://doi.org/10.48550/arXiv.2310.09754) , <br> by *Huanhuan Ma and
Weizhi Xu and
Yifan Wei and
Liuji Chen and
Liang Wang and
Qiang Liu and
Shu Wu and
Liang Wang* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L254-L275)<br> ```We present EX-FEVER, a new dataset for multi-hop explainable fact verification. EX-FEVER contains 10,000 claims and 100,000 abstracts from the research literature, and requires models to select abstracts containing evidence that SUPPORTS or REFUTES a given scientific claim. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2310-09754```<br>

## Bailey Kuehl

- [![](https://img.shields.io/badge/Findings_of_EMNLP-2022-blue)](https://doi.org/10.18653/v1/2022.findings-emnlp.347) [**SciFact-Open: Towards open-domain scientific claim verification**](https://doi.org/10.18653/v1/2022.findings-emnlp.347) , <br> by *David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Iz Beltagy and
Lucy Lu Wang and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L129-L149)<br> ```Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLKCBWH22```<br>

- [![](https://img.shields.io/badge/ACL-2022-blue)](https://doi.org/10.18653/v1/2022.acl-long.175) [**Generating Scientific Claims for Zero-Shot Scientific Fact Checking**](https://doi.org/10.18653/v1/2022.acl-long.175) , <br> by *Dustin Wright and
David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Isabelle Augenstein and
Lucy Lu Wang* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L213-L233)<br> </details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```0001WLKCAW22```<br>

## Florian Matthes

- [![](https://img.shields.io/badge/EACL-2024-blue)](https://doi.org/10.48550/arXiv.2402.02844) [**Comparing Knowledge Sources for Open-Domain Scientific Claim Verification**](https://doi.org/10.48550/arXiv.2402.02844) , <br> by *Juraj Vladika and
Florian Matthes* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L195-L210)<br> ```We compare knowledge sources for open-domain scientific claim verification, a task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2402-02844```<br>

- [![](https://img.shields.io/badge/Findings_of_ACL-2023-blue)](https://doi.org/10.18653/v1/2023.findings-acl.387) [**Scientific Fact-Checking: A Survey of Resources and Approaches**](https://doi.org/10.18653/v1/2023.findings-acl.387) , <br> by *Juraj Vladika and
Florian Matthes* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L90-L105)<br> ```We present a survey of resources and approaches for scientific fact-checking, a task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```VladikaM23```<br>

## Juraj Vladika

- [![](https://img.shields.io/badge/EACL-2024-blue)](https://doi.org/10.48550/arXiv.2402.02844) [**Comparing Knowledge Sources for Open-Domain Scientific Claim Verification**](https://doi.org/10.48550/arXiv.2402.02844) , <br> by *Juraj Vladika and
Florian Matthes* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L195-L210)<br> ```We compare knowledge sources for open-domain scientific claim verification, a task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2402-02844```<br>

- [![](https://img.shields.io/badge/Findings_of_ACL-2023-blue)](https://doi.org/10.18653/v1/2023.findings-acl.387) [**Scientific Fact-Checking: A Survey of Resources and Approaches**](https://doi.org/10.18653/v1/2023.findings-acl.387) , <br> by *Juraj Vladika and
Florian Matthes* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L90-L105)<br> ```We present a survey of resources and approaches for scientific fact-checking, a task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```VladikaM23```<br>

## Iz Beltagy

- [![](https://img.shields.io/badge/Findings_of_NAACL-2022-blue)](https://doi.org/10.18653/v1/2022.findings-naacl.6) [**MultiVerS: Improving scientific claim verification with weak supervision
and full-document context**](https://doi.org/10.18653/v1/2022.findings-naacl.6) , <br> by *David Wadden and
Kyle Lo and
Lucy Lu Wang and
Arman Cohan and
Iz Beltagy and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L48-L68)<br> ```We introduce MultiVerS, a weakly supervised model for scientific claim verification that leverages full-document context. We show that MultiVerS outperforms strong supervised baselines on the SciFact dataset, achieving state-of-the-art performance on this task. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLWCBH22```<br>

- [![](https://img.shields.io/badge/Findings_of_EMNLP-2022-blue)](https://doi.org/10.18653/v1/2022.findings-emnlp.347) [**SciFact-Open: Towards open-domain scientific claim verification**](https://doi.org/10.18653/v1/2022.findings-emnlp.347) , <br> by *David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Iz Beltagy and
Lucy Lu Wang and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L129-L149)<br> ```Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLKCBWH22```<br>
