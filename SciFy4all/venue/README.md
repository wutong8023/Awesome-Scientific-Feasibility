# Scientific Feasibility (SciFy) Literature 
This repository is maintained by [Tongtong Wu](https://wutong8023.site). Please don't hesitate to send me an email to collaborate or fix some entries (wutong8023 AT gmail.com). 
The automation script of this repo is powered by [Auto-Bibfile](https://github.com/wutong8023/Auto-Bibfile.git).

You can directly use our bibtex.bib in overleaf with this [link](https://www.overleaf.com/read/rgscdxhxbwhp).

This page categorizes the literature by the **Published Venue**.

## Outline 
- [![](https://img.shields.io/badge/Hyperlink-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue/README.md#hyperlink)
- [![](https://img.shields.io/badge/ACL-3-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue/README.md#acl)
- [![](https://img.shields.io/badge/EMNLP-5-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue/README.md#emnlp)
- [![](https://img.shields.io/badge/NAACL-1-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue/README.md#naacl)
- [![](https://img.shields.io/badge/EACL-1-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue/README.md#eacl)
- [![](https://img.shields.io/badge/SIGIR-2-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue/README.md#sigir)
- [![](https://img.shields.io/badge/LOUHI@EACL-1-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue/README.md#louhi@eacl)
- [![](https://img.shields.io/badge/Psychological_review-1-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue/README.md#psychological-review)
- [![](https://img.shields.io/badge/The_Oxford_Handbook_of_Thinking_and_Reasoning-1-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue/README.md#the-oxford-handbook-of-thinking-and-reasoning)
- [![](https://img.shields.io/badge/arXiv-11-blue)](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue/README.md#arxiv)
## Hyperlink 
 - [Overview](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/./)
 - [Application Area](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/application)
 - [Top Author](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/author)
 - [Contribution](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/contribution)
 - [Dataset Format](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/dataset)
 - [Foundation Model](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/foundation_model)
 - [Research Question](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/research_question)
 - [Published Time](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/time)
 - [Published Venue](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/SciFy4all/venue)

## ACL

- [![](https://img.shields.io/badge/Findings_of_ACL-2023-blue)](https://doi.org/10.18653/v1/2023.findings-acl.387) [**Scientific Fact-Checking: A Survey of Resources and Approaches**](https://doi.org/10.18653/v1/2023.findings-acl.387) , <br> by *Juraj Vladika and
Florian Matthes* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L90-L105)<br> ```We present a survey of resources and approaches for scientific fact-checking, a task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```VladikaM23```<br>

- [![](https://img.shields.io/badge/ACL-2022-blue)](https://doi.org/10.18653/v1/2022.acl-long.175) [**Generating Scientific Claims for Zero-Shot Scientific Fact Checking**](https://doi.org/10.18653/v1/2022.acl-long.175) , <br> by *Dustin Wright and
David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Isabelle Augenstein and
Lucy Lu Wang* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L213-L233)<br> </details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```0001WLKCAW22```<br>

- [![](https://img.shields.io/badge/ACL-2019-blue)](https://doi.org/10.18653/v1/p19-1244) [**Sentence-Level Evidence Embedding for Claim Verification with Hierarchical
Attention Networks**](https://doi.org/10.18653/v1/p19-1244) , <br> by *Jing Ma and
Wei Gao and
Shafiq R. Joty and
Kam{-}Fai Wong* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L364-L383)<br> ```We propose a hierarchical attention network for scientific claim verification that uses sentence-level evidence embeddings to make predictions. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```MaGJW19```<br>

## EMNLP

- [![](https://img.shields.io/badge/Findings_of_EMNLP-2023-blue)](https://aclanthology.org/2023.findings-emnlp.416) [**Explainable Claim Verification via Knowledge-Grounded Reasoning with
Large Language Models**](https://aclanthology.org/2023.findings-emnlp.416) , <br> by *Haoran Wang and
Kai Shu* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L235-L251)<br> ```We propose a knowledge-grounded reasoning model for scientific claim verification that uses large language models to generate explanations for the predictions. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WangS23a```<br>

- [![](https://img.shields.io/badge/EMNLP-2023-blue)](https://aclanthology.org/2023.emnlp-main.483) [**SCITAB: A Challenging Benchmark for Compositional Reasoning and
Claim Verification on Scientific Tables**](https://aclanthology.org/2023.emnlp-main.483) , <br> by *Xinyuan Lu and
Liangming Pan and
Qian Liu and
Preslav Nakov and
Min{-}Yen Kan* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L278-L297)<br> ```We present SCITAB, a new benchmark for compositional reasoning and claim verification on scientific tables. SCITAB contains 10,000 claims and 100,000 tables from the research literature, and requires models to select tables containing evidence that SUPPORTS or REFUTES a given scientific claim. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```LuPLNK23```<br>

- [![](https://img.shields.io/badge/Findings_of_EMNLP-2022-blue)](https://doi.org/10.18653/v1/2022.findings-emnlp.347) [**SciFact-Open: Towards open-domain scientific claim verification**](https://doi.org/10.18653/v1/2022.findings-emnlp.347) , <br> by *David Wadden and
Kyle Lo and
Bailey Kuehl and
Arman Cohan and
Iz Beltagy and
Lucy Lu Wang and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L129-L149)<br> ```Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLKCBWH22```<br>

- [![](https://img.shields.io/badge/EMNLP-2021-blue)](https://doi.org/10.18653/v1/2021.emnlp-main.290) [**Abstract, Rationale, Stance: A Joint Model for Scientific Claim
Verification**](https://doi.org/10.18653/v1/2021.emnlp-main.290) , <br> by *Zhiwei Zhang and
Jiyi Li and
Fumiyo Fukumoto and
Yanming Ye* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L108-L126)<br> ```We propose a joint model for scientific claim verification that predicts the stance of a given scientific claim by jointly learning to predict the abstract and rationale that support the stance. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```ZhangLFY21```<br>

- [![](https://img.shields.io/badge/EMNLP-2020-blue)](https://doi.org/10.18653/v1/2020.emnlp-main.609) [**Fact or Fiction: Verifying Scientific Claims**](https://doi.org/10.18653/v1/2020.emnlp-main.609) , <br> by *David Wadden and
Shanchuan Lin and
Kyle Lo and
Lucy Lu Wang and
Madeleine van Zuylen and
Arman Cohan and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L25-L45)<br> ```We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLLWZCH20```<br>

## NAACL

- [![](https://img.shields.io/badge/Findings_of_NAACL-2022-blue)](https://doi.org/10.18653/v1/2022.findings-naacl.6) [**MultiVerS: Improving scientific claim verification with weak supervision
and full-document context**](https://doi.org/10.18653/v1/2022.findings-naacl.6) , <br> by *David Wadden and
Kyle Lo and
Lucy Lu Wang and
Arman Cohan and
Iz Beltagy and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L48-L68)<br> ```We introduce MultiVerS, a weakly supervised model for scientific claim verification that leverages full-document context. We show that MultiVerS outperforms strong supervised baselines on the SciFact dataset, achieving state-of-the-art performance on this task. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```WaddenLWCBH22```<br>

## EACL

- [![](https://img.shields.io/badge/EACL-2024-blue)](https://doi.org/10.48550/arXiv.2402.02844) [**Comparing Knowledge Sources for Open-Domain Scientific Claim Verification**](https://doi.org/10.48550/arXiv.2402.02844) , <br> by *Juraj Vladika and
Florian Matthes* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L195-L210)<br> ```We compare knowledge sources for open-domain scientific claim verification, a task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2402-02844```<br>

## SIGIR

- [![](https://img.shields.io/badge/SIGIR-2023-blue)](https://doi.org/10.1145/3539618.3592049) [**Read it Twice: Towards Faithfully Interpretable Fact Verification
by Revisiting Evidence**](https://doi.org/10.1145/3539618.3592049) , <br> by *Xuming Hu and
Zhaochen Hong and
Zhijiang Guo and
Lijie Wen and
Philip S. Yu* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L321-L340)<br> ```In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```HuHGWY23```<br>

- [![](https://img.shields.io/badge/SIGIR-2023-blue)](https://doi.org/10.1145/3539618.3591879) [**End-to-End Multimodal Fact-Checking and Explanation Generation: A
Challenging Dataset and Models**](https://doi.org/10.1145/3539618.3591879) , <br> by *Barry Menglong Yao and
Aditya Shah and
Lichao Sun and
Jin{-}Hee Cho and
Lifu Huang* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L405-L424)<br> ```We present a challenging dataset and models for end-to-end multimodal fact-checking and explanation generation. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```YaoS0CH23```<br>

## LOUHI@EACL

- [![](https://img.shields.io/badge/LOUHI@EACL-2021-blue)](https://www.aclweb.org/anthology/2021.louhi-1.11/) [**Scientific Claim Verification with VerT5erini**](https://www.aclweb.org/anthology/2021.louhi-1.11/) , <br> by *Ronak Pradeep and
Xueguang Ma and
Rodrigo Frassetto Nogueira and
Jimmy Lin* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L4-L9)<br> ```We propose a system called VerT5erini that exploits T5 for abstract retrieval, sentence selection, and label prediction, which are three critical sub-tasks of claim verification. We evaluate our pipeline on SciFACT, a newly curated dataset that requires models to not just predict the veracity of claims but also provide relevant sentences from a corpus of scientific literature that support the prediction. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```PradeepMNL21```<br>

## Psychological review

- [![](https://img.shields.io/badge/Psychological_review-2015-blue)](https://psycnet.apa.org/record/2015-45017-003) [**An improved probabilistic account of counterfactual reasoning.**](https://psycnet.apa.org/record/2015-45017-003) , <br> by *Lucas, Christopher G and Kemp, Charles* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L475-L491)<br> ```When people want to identify the causes of an event, assign credit or blame, or learn from their mistakes, they often reflect on how things could have gone differently. In this kind of reasoning, one considers a counterfactual world in which some events are different from their real-world counterparts and considers what else would have changed. Researchers have recently proposed several probabilistic models that aim to capture how people do (or should) reason about counterfactuals. We present a new model and show that it accounts better for human inferences than several alternative models. Our model builds on the work of Pearl (2000), and extends his approach in a way that accommodates backtracking inferences and that acknowledges the difference between counterfactual interventions and counterfactual observations. We present 6 new experiments and analyze data from 4 experiments carried out by Rips (2010), and the results suggest that the new model provides an accurate account of both mean human judgments and the judgments of individuals.(PsycINFO Database Record (c) 2016 APA, all rights reserved)
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```lucas2015improved```<br>

## The Oxford Handbook of Thinking and Reasoning

- [![](https://img.shields.io/badge/The_Oxford_Handbook_of_Thinking_and_Reasoning-2012-blue)](https://doi.org/10.1093/oxfordhb/9780199734689.001.0001) [**Thinking and Reasoning: A Reader's Guide**](https://doi.org/10.1093/oxfordhb/9780199734689.001.0001) , <br> by *Holyoak, Keith J. and Morrison, Robert G.* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L427-L433)<br> ```This introductory article begins by asking: What is thinking? It looks at the various meanings of the concept in linguistic and philosophical terms. It summarizes the history of the academic study of thinking and reasoning. Finally it gives an outline of the six parts of the book which look in turn at general approaches to thinking and reasoning; inductive, deductive, and abductive reasoning; problem solving, intelligence, and creative thinking; judgment and decision making; ontogeny, phylogeny, language, and culture; and finally modes of thinking.
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```9780199734689```<br>

## arXiv

- [![](https://img.shields.io/badge/CoRR-2024-blue)](https://doi.org/10.48550/arXiv.2401.06853) [**Large Language Models Can Learn Temporal Reasoning**](https://doi.org/10.48550/arXiv.2401.06853) , <br> by *Siheng Xiong and
Ali Payani and
Ramana Kompella and
Faramarz Fekri* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L495-L511)<br> ```While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal expressions and intricate contextual details. In this paper, we propose TG-LLM, a new framework towards language-based TR. To be specific, we first teach LLM to translate the context into a temporal graph (TG). A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for fine-tuning on this graph translation task. We confirm in experiments that the capability of TG extraction learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we guide LLM to perform symbolic reasoning over the TG via Chain of Thoughts (CoTs) bootstrapping and special data augmentation strategies. We observe that CoTs with symbolic reasoning bring more consistent and reliable results than those using free-form text.
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2401-06853```<br>

- [![](https://img.shields.io/badge/CoRR-2023-blue)](https://doi.org/10.48550/arXiv.2310.09754) [**EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification**](https://doi.org/10.48550/arXiv.2310.09754) , <br> by *Huanhuan Ma and
Weizhi Xu and
Yifan Wei and
Liuji Chen and
Liang Wang and
Qiang Liu and
Shu Wu and
Liang Wang* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L254-L275)<br> ```We present EX-FEVER, a new dataset for multi-hop explainable fact verification. EX-FEVER contains 10,000 claims and 100,000 abstracts from the research literature, and requires models to select abstracts containing evidence that SUPPORTS or REFUTES a given scientific claim. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2310-09754```<br>

- [![](https://img.shields.io/badge/CoRR-2023-blue)](https://doi.org/10.48550/arXiv.2305.11859) [**Complex Claim Verification with Evidence Retrieved in the Wild**](https://doi.org/10.48550/arXiv.2305.11859) , <br> by *Jifan Chen and
Grace Kim and
Aniruddh Sriram and
Greg Durrett and
Eunsol Choi* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L300-L318)<br> ```In this work, we present the first fully automated pipeline to check real-world claims by retrieving raw evidence from the web. We restrict our retriever to only search documents available prior to the claim's making, modeling the realistic scenario where an emerging claim needs to be checked. Our pipeline includes five components: claim decomposition, raw document retrieval, fine-grained evidence retrieval, claim-focused summarization, and veracity judgment.
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2305-11859```<br>

- [![](https://img.shields.io/badge/CoRR-2023-blue)](https://doi.org/10.48550/arXiv.2304.02769) [**Low-Shot Learning for Fictional Claim Verification**](https://doi.org/10.48550/arXiv.2304.02769) , <br> by *Viswanath Chadalapaka and
Derek Nguyen and
JoonWon Choi and
Shaunak Joshi and
Mohammad Rostami* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L343-L361)<br> ```We propose a low-shot learning model for fictional claim verification that uses a small number of examples to make predictions. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2304-02769```<br>

- [![](https://img.shields.io/badge/CoRR-2023-blue)](https://doi.org/10.48550/arXiv.2301.08914) [**ExClaim: Explainable Neural Claim Verification Using Rationalization**](https://doi.org/10.48550/arXiv.2301.08914) , <br> by *Sai Gurrapu and
Lifu Huang and
Feras A. Batarseh* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L386-L402)<br> ```We propose an explainable neural claim verification model that uses rationalization to generate explanations for the predictions. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2301-08914```<br>

- [![](https://img.shields.io/badge/CoRR-2023-blue)](https://doi.org/10.48550/arXiv.2305.15541) [**Harnessing the Power of Large Language Models for Natural Language
to First-Order Logic Translation**](https://doi.org/10.48550/arXiv.2305.15541) , <br> by *Yuan Yang and
Siheng Xiong and
Ali Payani and
Ehsan Shareghi and
Faramarz Fekri* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L514-L533)<br> ```Translating natural language sentences to first-order logic (NL-FOL translation) is a longstanding challenge in the NLP and formal logic literature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for NL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of directly translating natural language into FOL rules, which outperforms GPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5, and can achieve similar performance as GPT-4 with a fraction of the cost. This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model. To train LogicLLaMA, we present MALLS (large language Model generAted NL-FOL pairS), a dataset of 34K high-quality and diverse sentence-level NL-FOL pairs collected from GPT-4. The dataset was created by implementing a pipeline that prompts GPT-4 for pairs, and dynamically adjusts the prompts to ensure the collection of pairs with rich and diverse contexts at different levels of complexity, and verifies the validity of the generated FOL rules.
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2305-15541```<br>

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2202.02646) [**RerrFact: Reduced Evidence Retrieval Representations for Scientific
Claim Verification**](https://arxiv.org/abs/2202.02646) , <br> by *Ashish Rana and
Deepanshu Khanna and
Muskaan Singh and
Tirthankar Ghosal and
Harpreet Singh and
Prashant Singh Rana* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L152-L172)<br> ```We propose a reduced evidence retrieval model for scientific claim verification that uses a small number of retrieved abstracts to make predictions. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2202-02646```<br>

- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2107.08188) [**Overview and Insights from the SciVer Shared Task on Scientific Claim
Verification**](https://arxiv.org/abs/2107.08188) , <br> by *David Wadden and
Kyle Lo* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L71-L87)<br> ```We present an overview of the SciVer shared task on scientific claim verification, which was held as part of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). The shared task aimed to promote research on scientific claim verification, a new task that requires models to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2107-08188```<br>

- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2104.11572) [**QMUL-SDS at SCIVER: Step-by-Step Binary Classification for Scientific
Claim Verification**](https://arxiv.org/abs/2104.11572) , <br> by *Xia Zeng and
Arkaitz Zubiaga* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L175-L191)<br> ```We present a step-by-step binary classification model for scientific claim verification that predicts the stance of a given scientific claim by learning to predict the abstract and rationale that support the stance. 
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2104-11572```<br>

- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2106.11417) [**Interpretable Model-based Hierarchical Reinforcement Learning using
Inductive Logic Programming**](https://arxiv.org/abs/2106.11417) , <br> by *Duo Xu and
Faramarz Fekri* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L555-L570)<br> ```Recently deep reinforcement learning has achieved tremendous success in wide ranges of applications. However, it notoriously lacks data-efficiency and interpretability. Data-efficiency is important as interacting with the environment is expensive. Further, interpretability can increase the transparency of the black-box-style deep RL models and hence gain trust from the users. In this work, we propose a new hierarchical framework via symbolic RL, leveraging a symbolic transition model to improve the data-efficiency and introduce the interpretability for learned policy. This framework consists of a high-level agent, a subtask solver and a symbolic transition model. Without assuming any prior knowledge on the state transition, we adopt inductive logic programming (ILP) to learn the rules of symbolic state transitions, introducing interpretability and making the learned behavior understandable to users. In empirical experiments, we confirmed that the proposed framework offers approximately between 30\% to 40\% more data efficiency over previous methods.
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-2106-11417```<br>

- [![](https://img.shields.io/badge/CoRR-2019-blue)](http://arxiv.org/abs/1904.01554) [**Learning Algorithms via Neural Logic Networks**](http://arxiv.org/abs/1904.01554) , <br> by *Ali Payani and
Faramarz Fekri* [[bib]](https://github.com/wutong8023/Awesome-Scientific-Feasibility/blob/master/./bibtex.bib#L573-L587)<br> ```We propose a novel learning paradigm for Deep Neural Networks (DNN) by using Boolean logic algebra. We first present the basic differentiable operators of a Boolean system such as conjunction, disjunction and exclusive-OR and show how these elementary operators can be combined in a simple and meaningful way to form Neural Logic Networks (NLNs). We examine the effectiveness of the proposed NLN framework in learning Boolean functions and discrete-algorithmic tasks. We demonstrate that, in contrast to the implicit learning in MLP approach, the proposed neural logic networks can learn the logical functions explicitly that can be verified and interpreted by human. In particular, we propose a new framework for learning the inductive logic programming (ILP) problems by exploiting the explicit representational power of NLN. We show the proposed neural ILP solver is capable of feats such as predicate invention and recursion and can outperform the current state of the art neural ILP solvers using a variety of benchmark tasks such as decimal addition and multiplication, and sorting on ordered list.
```</details><details><summary><i class="fa-solid fa-bars"></i></summary><pre>```abs-1904-01554```<br>
